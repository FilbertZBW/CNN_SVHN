{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. ## Import SVHN dataset\n",
    "2. ## Data preprocessing\n",
    "   - Check data type\n",
    "   - Convert to float for image and to int for label\n",
    "   - Normalize the data\n",
    "   - One-hot encoding for labels\n",
    "   - Data augmentation\n",
    "      > augment the images in the dataset, by randomly rotating them, zooming them in and out, shifting them up and down\n",
    " \n",
    " <br>\n",
    " \n",
    " 3. ## Create CNN model\n",
    "\n",
    " 4. ## Train & Test\n",
    "\n",
    " 5. ## Results and discussion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbox\n",
      "name\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import h5py\n",
    "\n",
    "# data is loaded from local path, please update local_path according to your own dataset location\n",
    "local_path = \"C:/Users/kaili/OneDrive/Desktop/CS6140 ML/Project Dataset/\"\n",
    "\n",
    "# load digitStruct.mat\n",
    "data_path = local_path + \"train/digitStruct.mat\"\n",
    "f = h5py.File(data_path, 'r')\n",
    "\n",
    "def printname(name):\n",
    "    print(name)\n",
    "\n",
    "# get names of h5py groups within digitStruct\n",
    "f['digitStruct'].visit(printname) # digitStruct.mat contains information of bbox, name of the images\n",
    "# ref: https://docs.h5py.org/en/stable/quick.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get dataset for each group\n",
    "digitStructName = f['digitStruct']['name']\n",
    "digitStructBbox = f['digitStruct']['bbox']\n",
    "\n",
    "# get bbox for each instance. bbox is a library describing the bounder box position\n",
    "def get_bbox(n):\n",
    "    bbox = {}\n",
    "    bb = digitStructBbox[n].item()\n",
    "    def bbox_helper(attr_type):\n",
    "        attr = f[bb][attr_type]\n",
    "        if len(attr) > 1:\n",
    "            pos = [int(f[attr[j].item()][0][0]) for j in range(len(attr))]\n",
    "        else:\n",
    "            pos = [attr[0][0]]\n",
    "\n",
    "        return pos\n",
    "    bbox['label'] = bbox_helper('label')\n",
    "    bbox['height'] = bbox_helper('height')\n",
    "    bbox['left'] = bbox_helper('left')\n",
    "    bbox['top'] = bbox_helper('top')\n",
    "    bbox['width'] = bbox_helper('width')\n",
    "    return bbox\n",
    "\n",
    "# get file name for each instance. File names are indexes starting from 1\n",
    "def get_name(n):\n",
    "    name = ''.join([chr(v[0]) for v in f[(digitStructName[n][0])]])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 2, 10], 'height': [37, 37, 37], 'left': [50, 66, 85], 'top': [7, 7, 11], 'width': [15, 21, 27]}\n",
      "98.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test using first image\n",
    "print(get_bbox(97))\n",
    "print(get_name(97))\n",
    "from IPython.display import Image\n",
    "img_path_test = local_path + \"/train/98.png\"\n",
    "Image(filename=img_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# store bbox and file name into dict\n",
    "# image_dict = {}\n",
    "# for i in range(len(digitStructName)):\n",
    "#     file_name = get_name(i)\n",
    "#     image_dict[file_name] = get_bbox(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# test using first image\n",
    "#digit_dict = image_dict['1.png']\n",
    "digit_dict = get_bbox(0)\n",
    "image_name = get_name(0)\n",
    "img_path_crop_test = local_path + 'train/' + image_name\n",
    "image = cv2.imread(img_path_crop_test)\n",
    "n_drop = 0\n",
    "\n",
    "# for each label, crop the image\n",
    "for label_index in range(len(digit_dict['label'])):\n",
    "    label = int(digit_dict['label'][label_index])\n",
    "    # label 0 is represented using 10\n",
    "    if label == 10:\n",
    "        label = 0\n",
    "\n",
    "    # get crop position\n",
    "    left = int(digit_dict['left'][label_index])\n",
    "    upper = int(digit_dict['top'][label_index])\n",
    "    right = int(left + digit_dict['width'][label_index])\n",
    "    lower = int(upper + digit_dict['height'][label_index])\n",
    "\n",
    "    # invalid data\n",
    "    if left < 0 or upper < 0:\n",
    "        n_drop += 1\n",
    "        continue\n",
    "\n",
    "    # crop image\n",
    "    img = image[upper:lower, left:right, :]\n",
    "\n",
    "    # transpose image into size 32 * 32 grayscale tensor format\n",
    "    transformed_img = transforms.Compose(\n",
    "            [transforms.ToPILImage(),\n",
    "             transforms.Grayscale(1),\n",
    "             transforms.Resize((32, 32)),\n",
    "             transforms.ToTensor()])(img)\n",
    "    # ref: http://man.hubwiz.com/docset/torchvision.docset/Contents/Resources/Documents/transforms.html\n",
    "\n",
    "    process_folder = local_path + 'process/'\n",
    "    # check whether the process folder exists or not\n",
    "    if not os.path.exists(process_folder):\n",
    "        # create a new directory because it does not exist\n",
    "        os.makedirs(process_folder)\n",
    "        print(\"Created the process folder to store preprocessed images\")\n",
    "\n",
    "    # store image as original file name + _ + label + _ + label index, saved in process folder\n",
    "    save_file_path = '_'.join([process_folder + image_name.split('.')[0], str(label_index), str(label) +'.png'])\n",
    "\n",
    "    # save cropped image\n",
    "    torchvision.utils.save_image(transformed_img, save_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE RUN FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following class are used for image crop and get cropped image path \n",
    "class CropImage(object):\n",
    "    def __init__ (self, local_path, mode):\n",
    "        # load digitStruct.mat\n",
    "        self.local_path = local_path\n",
    "        self.mode = mode\n",
    "        if mode == \"train\":\n",
    "            self.f = h5py.File(local_path + \"/train/digitStruct.mat\", 'r')\n",
    "        elif mode == \"test\":\n",
    "            self.f = h5py.File(local_path + \"/test/digitStruct.mat\", 'r')\n",
    "        # get dataset for each group\n",
    "        self.digitStructName = self.f['digitStruct']['name']\n",
    "        self.digitStructBbox = self.f['digitStruct']['bbox']\n",
    "        self.file_path_dict = {}\n",
    "        \n",
    "    # get bbox for each instance. bbox is a library describing the bounder box position\n",
    "    def get_bbox(self, n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        def bbox_helper(attr_type):\n",
    "            attr = self.f[bb][attr_type]\n",
    "            if len(attr) > 1:\n",
    "                pos = [int(self.f[attr[j].item()][0][0]) for j in range(len(attr))]\n",
    "            else:\n",
    "                pos = [attr[0][0]]\n",
    "\n",
    "            return pos\n",
    "        bbox['label'] = bbox_helper('label')\n",
    "        bbox['height'] = bbox_helper('height')\n",
    "        bbox['left'] = bbox_helper('left')\n",
    "        bbox['top'] = bbox_helper('top')\n",
    "        bbox['width'] = bbox_helper('width')\n",
    "        return bbox\n",
    "\n",
    "    # get file name for each instance. File names are indexes starting from 1\n",
    "    def get_name(self, n):\n",
    "        name = ''.join([chr(v[0]) for v in self.f[(self.digitStructName[n][0])]])\n",
    "        return name\n",
    "\n",
    "    def save_path_to_dict(self, original_file_name, save_file_path, img):\n",
    "        # save path to dict\n",
    "        if original_file_name not in self.file_path_dict:\n",
    "            self.file_path_dict[original_file_name] = [save_file_path]\n",
    "        elif save_file_path not in self.file_path_dict[original_file_name]:\n",
    "            self.file_path_dict[original_file_name].append(save_file_path)\n",
    "        # save cropped image\n",
    "        torchvision.utils.save_image(img, save_file_path)\n",
    "\n",
    "    # crop image for each instance, also save cropped image name to the dict\n",
    "    def crop_image(self, n):\n",
    "        n_drop = 0\n",
    "        # for each label, crop the image\n",
    "        digit_dict = self.get_bbox(n)\n",
    "        \n",
    "        for label_index in range(len(digit_dict['label'])):\n",
    "            label = int(digit_dict['label'][label_index])\n",
    "            # label 0 is represented using 10\n",
    "            if label == 10:\n",
    "                label = 0\n",
    "\n",
    "            # get crop position\n",
    "            left = int(digit_dict['left'][label_index])\n",
    "            upper = int(digit_dict['top'][label_index])\n",
    "            right = int(left + digit_dict['width'][label_index])\n",
    "            lower = int(upper + digit_dict['height'][label_index])\n",
    "\n",
    "            # invalid data\n",
    "            if left < 0 or upper < 0:\n",
    "                n_drop += 1\n",
    "                continue\n",
    "\n",
    "            # crop image\n",
    "            img_path_crop_test = self.local_path + '/train/' + self.get_name(n)\n",
    "            image = cv2.imread(img_path_crop_test)\n",
    "            img = image[upper:lower, left:right, :]\n",
    "\n",
    "            # transpose image into size 32 * 32 grayscale tensor format\n",
    "            transformed_img = transforms.Compose(\n",
    "                    [transforms.ToPILImage(),\n",
    "                     transforms.Grayscale(1),\n",
    "                     transforms.Resize((32, 32)),\n",
    "                     transforms.ToTensor()])(img)\n",
    "            # ref: http://man.hubwiz.com/docset/torchvision.docset/Contents/Resources/Documents/transforms.html\n",
    "\n",
    "            process_folder = self.local_path + '/process/'\n",
    "            # check whether the process folder exists or not\n",
    "            if not os.path.exists(process_folder):\n",
    "                # create a new directory because it does not exist\n",
    "                os.makedirs(process_folder)\n",
    "                print(\"Created the process folder to store preprocessed images\")\n",
    "\n",
    "            # store image as original file name + _ + label + _ + label index, saved in process folder\n",
    "            original_file_name = self.get_name(n).split('.')[0]\n",
    "\n",
    "            file_head = '_'.join([process_folder + original_file_name, str(label_index), str(label)])\n",
    "            file_tail = '.png'\n",
    "            inverted_file_tail = '_iv.png'\n",
    "\n",
    "            # invert grayscale image to get a new image, by doing so we could expande our training data set\n",
    "            if self.mode == \"train\":\n",
    "                inverted_transformed_img = transforms.functional.invert(transformed_img)\n",
    "                self.save_path_to_dict(original_file_name, file_head+inverted_file_tail, inverted_transformed_img)\n",
    "        \n",
    "            self.save_path_to_dict(original_file_name, file_head+file_tail, transformed_img)\n",
    "    \n",
    "    # get a list of cropped image file path\n",
    "    def get_cropped_image_path(self, n):\n",
    "        curr_path = self.local_path + \"/train\" if self.mode == \"train\" else self.local_path + \"/test\"\n",
    "        # check the file name is out of range, max is 33401 for train, 13065 for test\n",
    "        if n + 1 not in self.file_path_dict:\n",
    "            if n + 3 >= len(os.listdir(curr_path)):\n",
    "                print(\"Please check the file name!\", n) ### changed\n",
    "            else:\n",
    "                # havent been processed yet\n",
    "                self.crop_image(n)\n",
    "        # return a list of file path name of the cropped file \n",
    "        return self.file_path_dict[str(n+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nci_train = CropImage(local_path=\"../dataset\", mode=\"train\")\\nci_train.get_cropped_image_path(78)\\n\\nci_test = CropImage(local_path=\"../dataset\", mode=\"test\")\\nci_test.get_cropped_image_path(3)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functionality\n",
    "# replace \"local_path\" to your real local path before test, e.g. \"/Users/evelynhsu/CS6140\"\n",
    "'''\n",
    "ci_train = CropImage(local_path=\"../dataset\", mode=\"train\")\n",
    "ci_train.get_cropped_image_path(78)\n",
    "\n",
    "ci_test = CropImage(local_path=\"../dataset\", mode=\"test\")\n",
    "ci_test.get_cropped_image_path(3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_input(local_path, mode):\n",
    "    X, y = [], []\n",
    "    data = CropImage(local_path, mode)\n",
    "\n",
    "    dataset_path = local_path + \"/\" + mode\n",
    "    total_image_num = len(glob.glob1(dataset_path,\"*.png\"))\n",
    "    print(\"Total image num: \", total_image_num)\n",
    "\n",
    "    for idx in range(total_image_num): #####\n",
    "        image_filename = idx + 1\n",
    "\n",
    "        try:\n",
    "            cropped_img_dir_list = data.get_cropped_image_path(image_filename)\n",
    "\n",
    "            if len(cropped_img_dir_list) != len(data.get_bbox(image_filename)['label'] * 2):\n",
    "                print(\"1. Skipped, Number of cropped image is inconsistent with labels, img number:\", image_filename, data.get_bbox(image_filename)['label'])\n",
    "                continue\n",
    "\n",
    "            for cropped_img_dir in cropped_img_dir_list:\n",
    "                cropped_img = cv2.imread(cropped_img_dir)\n",
    "                X.append(cropped_img)\n",
    "\n",
    "            for l in data.get_bbox(image_filename)['label']:\n",
    "                if l == 10:\n",
    "                    y += [0] * 2\n",
    "                else:\n",
    "                    y += [l] * 2\n",
    "\n",
    "        except:\n",
    "            print(\"2. Skipped, No cropped image output, img number:\", image_filename)\n",
    "            continue\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = \"C:/Users/kaili/OneDrive/Desktop/CS6140 ML/Project Dataset\"\n",
    "# mode = \"train\"\n",
    "# img = 97\n",
    "# data = CropImage(local_path, mode)\n",
    "# cropped_img_dir_list = data.get_cropped_image_path(img) \n",
    "# print(cropped_img_dir_list)\n",
    "# print(data.get_bbox(img)['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image num:  33402\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 251 [3, 7, 10]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 723 [2, 7]\n",
      "2. Skipped, No cropped image output, img number: 1393\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 3099 [1, 2, 6]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 5311 [2, 10, 8]\n",
      "2. Skipped, No cropped image output, img number: 6876\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 9356 [8, 2, 5]\n",
      "2. Skipped, No cropped image output, img number: 11860\n",
      "2. Skipped, No cropped image output, img number: 13144\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 15124 [3, 5]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 18276 [1, 1]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 18392 [3, 1]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 19382 [1, 7]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 20539 [4, 8]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 20900 [9, 3]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 20914 [2, 7]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 22441 [1, 10, 6]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 23904 [1, 7]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 26569 [5, 1]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 27637 [3, 3]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 27808 [1, 5]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 28512 [1, 3]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 28797 [1, 5]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 28817 [2, 1]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 28849 [3, 1]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 29758 [1, 10]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 30509 [3, 2, 7]\n",
      "2. Skipped, No cropped image output, img number: 30952\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 32168 [6, 7]\n",
      "1. Skipped, Number of cropped image is inconsistent with labels, img number: 32431 [9, 7]\n",
      "Please check the file name! 33401\n",
      "2. Skipped, No cropped image output, img number: 33401\n",
      "Please check the file name! 33402\n",
      "2. Skipped, No cropped image output, img number: 33402\n",
      "(146378, 32, 32, 3) (146378,)\n"
     ]
    }
   ],
   "source": [
    "# Create Train datasets \n",
    "# ETA 38min\n",
    "local_path = \"C:/Users/kaili/OneDrive/Desktop/CS6140 ML/Project Dataset\"\n",
    "mode = \"train\"\n",
    "X_train, y_train = create_input(local_path, mode)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlocal_path = \"C:/Users/kaili/OneDrive/Desktop/CS6140 ML/Project Dataset\"\\nmode = \"test\"\\nimg = 2\\ndata2 = CropImage(local_path, mode)\\ncropped_img_dir_list = data2.get_cropped_image_path(img) \\nprint(cropped_img_dir_list)\\nprint(data2.get_bbox(img)[\\'label\\'])\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "local_path = \"C:/Users/kaili/OneDrive/Desktop/CS6140 ML/Project Dataset\"\n",
    "mode = \"test\"\n",
    "img = 2\n",
    "data2 = CropImage(local_path, mode)\n",
    "cropped_img_dir_list = data2.get_cropped_image_path(img) \n",
    "print(cropped_img_dir_list)\n",
    "print(data2.get_bbox(img)['label'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Test datasets\n",
    "# local_path = \"C:/Users/kaili/OneDrive/Desktop/CS6140 ML/Project Dataset\"\n",
    "# mode = \"test\"\n",
    "# X_test, y_test = create_input(local_path, mode)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential model from keras\n",
    "\n",
    "1. CNN Class - modify numbers & order of layers according to information loss?\n",
    "    e.g. \n",
    "    Class CNN():\n",
    "        def __init__(self, layer_num=None, layer_order=None):\n",
    "            pass\n",
    "\n",
    "2. for all training instance:\n",
    "        for all numbers in instance:\n",
    "            input to the current CNN model\n",
    "\n",
    "\n",
    "\n",
    "3. Model evaluation:\n",
    "    a. confusion matrix:\n",
    "        0  1  2  3  4 5 6 7 8 9\n",
    "    0\n",
    "    1\n",
    "    2\n",
    "    ...\n",
    "\n",
    "    b. Feature Maps - output of each layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "cnn_model = models.Sequential()\n",
    "cnn_model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu')) # w/o 0.35 accuracy 0.8949\n",
    "\n",
    "cnn_model.add(layers.Flatten()) \n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4575/4575 [==============================] - 68s 15ms/step - loss: 0.9349 - accuracy: 0.7447\n",
      "Epoch 2/10\n",
      "4575/4575 [==============================] - 67s 15ms/step - loss: 0.5635 - accuracy: 0.8367\n",
      "Epoch 3/10\n",
      "4575/4575 [==============================] - 67s 15ms/step - loss: 0.5002 - accuracy: 0.8553\n",
      "Epoch 4/10\n",
      "4575/4575 [==============================] - 66s 14ms/step - loss: 0.4655 - accuracy: 0.8638\n",
      "Epoch 5/10\n",
      "4575/4575 [==============================] - 66s 15ms/step - loss: 0.4378 - accuracy: 0.8709\n",
      "Epoch 6/10\n",
      "4575/4575 [==============================] - 67s 15ms/step - loss: 0.4182 - accuracy: 0.8768\n",
      "Epoch 7/10\n",
      "4575/4575 [==============================] - 67s 15ms/step - loss: 0.4026 - accuracy: 0.8813\n",
      "Epoch 8/10\n",
      "4575/4575 [==============================] - 68s 15ms/step - loss: 0.3886 - accuracy: 0.8857\n",
      "Epoch 9/10\n",
      "4575/4575 [==============================] - 65s 14ms/step - loss: 0.3783 - accuracy: 0.8881\n",
      "Epoch 10/10\n",
      "4575/4575 [==============================] - 68s 15ms/step - loss: 0.3680 - accuracy: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268f41c1490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit train data ~11min\n",
    "cnn_model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4575/4575 [==============================] - 24s 5ms/step - loss: 0.3359 - accuracy: 0.9002\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 167,562\n",
      "Trainable params: 167,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# cnn.evaluate(X_test,y_test)\n",
    "cnn_model.evaluate(X_train,y_train) # loss: 0.2900 - accuracy: 0.9131\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.evaluate(X_test,y_test) # loss: xxx - accuracy: xxx\n",
    "# y_pred = cnn_model.predict(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
