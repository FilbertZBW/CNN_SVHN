{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. ## Import SVHN dataset\n",
    "2. ## Data preprocessing\n",
    "   - Check data type\n",
    "   - Convert to float for image and to int for label\n",
    "   - Normalize the data\n",
    "   - One-hot encoding for labels\n",
    "   - Data augmentation\n",
    "      > augment the images in the dataset, by randomly rotating them, zooming them in and out, shifting them up and down\n",
    " \n",
    " <br>\n",
    " \n",
    " 3. ## Create CNN model\n",
    "\n",
    " 4. ## Train & Test\n",
    "\n",
    " 5. ## Results and discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN FROM HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = \"../..\" <- please change dir here\n",
    "local_path = \"../dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Crop Images and Convert to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following class are used for image crop and get cropped image path \n",
    "class CropImage(object):\n",
    "    def __init__ (self, local_path, mode, grayscale=True):\n",
    "        # load digitStruct.mat\n",
    "        self.local_path = local_path\n",
    "        self.mode = mode\n",
    "        self.grayscale = grayscale\n",
    "        if mode == \"train\":\n",
    "            self.f = h5py.File(local_path + \"/train/digitStruct.mat\", 'r')\n",
    "        elif mode == \"test\":\n",
    "            self.f = h5py.File(local_path + \"/test/digitStruct.mat\", 'r')\n",
    "        # get dataset for each group\n",
    "        self.digitStructName = self.f['digitStruct']['name']\n",
    "        self.digitStructBbox = self.f['digitStruct']['bbox']\n",
    "        self.file_path_dict = {}\n",
    "        \n",
    "    # get bbox for each instance. bbox is a library describing the bounder box position\n",
    "    def get_bbox(self, n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        def bbox_helper(attr_type):\n",
    "            attr = self.f[bb][attr_type]\n",
    "            if len(attr) > 1:\n",
    "                pos = [int(self.f[attr[j].item()][0][0]) for j in range(len(attr))]\n",
    "            else:\n",
    "                pos = [attr[0][0]]\n",
    "\n",
    "            return pos\n",
    "        bbox['label'] = bbox_helper('label')\n",
    "        bbox['height'] = bbox_helper('height')\n",
    "        bbox['left'] = bbox_helper('left')\n",
    "        bbox['top'] = bbox_helper('top')\n",
    "        bbox['width'] = bbox_helper('width')\n",
    "        return bbox\n",
    "\n",
    "    # get file name for each instance. File names are indexes starting from 1\n",
    "    def get_name(self, n):\n",
    "        name = ''.join([chr(v[0]) for v in self.f[(self.digitStructName[n][0])]])\n",
    "        return name\n",
    "\n",
    "    def save_path_to_dict(self, original_file_name, save_file_path, img):\n",
    "        # save path to dict\n",
    "        if original_file_name not in self.file_path_dict:\n",
    "            self.file_path_dict[original_file_name] = [save_file_path]\n",
    "        elif save_file_path not in self.file_path_dict[original_file_name]:\n",
    "            self.file_path_dict[original_file_name].append(save_file_path)\n",
    "        # save cropped image\n",
    "        torchvision.utils.save_image(img, save_file_path)\n",
    "\n",
    "    # crop image for each instance, also save cropped image name to the dict\n",
    "    def crop_image(self, n):\n",
    "        n_drop = 0\n",
    "        # for each label, crop the image\n",
    "        digit_dict = self.get_bbox(n)\n",
    "        \n",
    "        for label_index in range(len(digit_dict['label'])):\n",
    "            label = int(digit_dict['label'][label_index])\n",
    "            # label 0 is represented using 10\n",
    "            if label == 10:\n",
    "                label = 0\n",
    "\n",
    "            # get crop position\n",
    "            left = int(digit_dict['left'][label_index])\n",
    "            upper = int(digit_dict['top'][label_index])\n",
    "            right = int(left + digit_dict['width'][label_index])\n",
    "            lower = int(upper + digit_dict['height'][label_index])\n",
    "\n",
    "            # invalid data\n",
    "            if left < 0 or upper < 0:\n",
    "                n_drop += 1\n",
    "                continue\n",
    "\n",
    "            # crop image\n",
    "            img_path_crop_test = self.local_path + '/' + self.mode + '/' + self.get_name(n)\n",
    "            image = cv2.imread(img_path_crop_test)\n",
    "            img = image[upper:lower, left:right, :]\n",
    "\n",
    "            if self.grayscale == True:\n",
    "                # transpose image into size 32 * 32 grayscale tensor format\n",
    "                transformed_img = transforms.Compose(\n",
    "                        [transforms.ToPILImage(),\n",
    "                        transforms.Grayscale(num_output_channels=1),\n",
    "                        transforms.Resize((32, 32)),\n",
    "                        transforms.ToTensor()])(img)\n",
    "                # ref: http://man.hubwiz.com/docset/torchvision.docset/Contents/Resources/Documents/transforms.html\n",
    "            else:\n",
    "                 # If num_output_channels == 1 : returned image is single channel If num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
    "                transformed_img = transforms.Compose(\n",
    "                        [transforms.ToPILImage(),\n",
    "                        transforms.Resize((32, 32)),\n",
    "                        transforms.ToTensor()])(img)\n",
    "\n",
    "            grayscale_text = 'single' if self.grayscale == True else 'rgb'\n",
    "            process_folder = self.local_path + '/process/' + self.mode + '/' + grayscale_text +'/'\n",
    "            # check whether the process folder exists or not\n",
    "            if not os.path.exists(process_folder):\n",
    "                # create a new directory because it does not exist\n",
    "                os.makedirs(process_folder)\n",
    "                print(\"Created the process folder to store preprocessed images\")\n",
    "\n",
    "            # store image as original file name + _ + label + _ + label index, saved in process folder\n",
    "            original_file_name = self.get_name(n).split('.')[0]\n",
    "\n",
    "            file_head = '_'.join([process_folder + original_file_name, str(label_index), str(label)])\n",
    "            file_tail = '.png'\n",
    "            inverted_file_tail = '_iv.png'\n",
    "\n",
    "            # invert grayscale image to get a new image, by doing so we could expande our training data set\n",
    "            if self.mode == \"train\":\n",
    "                inverted_transformed_img = transforms.functional.invert(transformed_img)\n",
    "                self.save_path_to_dict(original_file_name, file_head+inverted_file_tail, inverted_transformed_img)\n",
    "        \n",
    "            self.save_path_to_dict(original_file_name, file_head+file_tail, transformed_img)\n",
    "    \n",
    "    # get a list of cropped image file path\n",
    "    def get_cropped_image_path(self, n):\n",
    "        curr_path = self.local_path + '/' + self.mode + '/'\n",
    "        # check the file name is out of range, max is 33401 for train, 13065 for test\n",
    "        if n + 1 not in self.file_path_dict:\n",
    "            if n + 2 >= len(os.listdir(curr_path)): # changed from 3 to 2\n",
    "                print(\"Please check the file name!\")\n",
    "            else:\n",
    "                # havent been processed yet\n",
    "                self.crop_image(n)\n",
    "        # return a list of file path name of the cropped file \n",
    "        return self.file_path_dict[str(n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # test functionality\n",
    "# # replace \"local_path\" to your real local path before test, e.g. \"/Users/evelynhsu/CS6140\"\n",
    "# ci_train = CropImage(local_path=local_path, mode=\"train\", grayscale=True)\n",
    "# ci_train.get_cropped_image_path(33401)\n",
    "# ci_test = CropImage(local_path=local_path, mode=\"test\", grayscale=False)\n",
    "# ci_test.get_cropped_image_path(13067)\n",
    "# ci_test = CropImage(local_path=local_path, mode=\"test\", grayscale=True)\n",
    "# ci_test.get_cropped_image_path(13067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_images(cropped_img_dir_list):\n",
    "    '''\n",
    "    Return a list of images of a single number\n",
    "    e.g. Original image = 210, return image of 2, 1, 0\n",
    "    '''\n",
    "    output = []\n",
    "    for cropped_img_dir in cropped_img_dir_list:\n",
    "        output.append(cv2.imread(cropped_img_dir))\n",
    "    return output\n",
    "\n",
    "def get_labels(data, idx, multiplier=1):\n",
    "    '''\n",
    "    Return a list of labels. Convert label 10 -> 0\n",
    "    '''\n",
    "    output = []\n",
    "    for l in data.get_bbox(idx)['label']:\n",
    "        output += [0] * multiplier if l == 10 else [int(l)]* multiplier\n",
    "    return output\n",
    "\n",
    "def create_input(local_path, mode, grayscale): # <- possible a better function name?\n",
    "    '''\n",
    "    Return lists of train/test\n",
    "    '''\n",
    "    X, y, image_number = [], [], []\n",
    "    data = CropImage(local_path, mode, grayscale)\n",
    "    dataset_path = local_path + \"/\" + mode\n",
    "    total_image_num = len(glob.glob1(dataset_path, \"*.png\"))\n",
    "\n",
    "    for idx in range(total_image_num):\n",
    "        try:\n",
    "            cropped_img_dir_list = data.get_cropped_image_path(idx)\n",
    "            # Two images per number in the train, and only one in the test\n",
    "            multiplier = 2 if mode == \"train\" else 1\n",
    "\n",
    "            # Check if number of Cropped img == number of labels\n",
    "            if len(cropped_img_dir_list) != len(\n",
    "                    data.get_bbox(idx)['label'] * multiplier):\n",
    "                print(\n",
    "                    \"Skipped {}.png, Number of cropped image is inconsistent with labels\"\n",
    "                    .format(idx + 1))\n",
    "                continue\n",
    "\n",
    "            X += get_cropped_images(cropped_img_dir_list)\n",
    "            y += get_labels(data, idx, multiplier)\n",
    "            image_number += [idx] * len(cropped_img_dir_list)\n",
    "\n",
    "        except:\n",
    "            # Failed to crop image\n",
    "            print(\"Skipped {}.png, No cropped image output\".format(idx + 1))\n",
    "            continue\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(image_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 252.png, Number of cropped image is inconsistent with labels\n",
      "Skipped 724.png, Number of cropped image is inconsistent with labels\n",
      "Skipped 815.png, No cropped image output\n"
     ]
    }
   ],
   "source": [
    "# Create grayscaled train and test datasets\n",
    "X_train, y_train, image_train = create_input(local_path, mode=\"train\", grayscale=True)\n",
    "X_test, y_test, img_number = create_input(local_path, mode=\"test\", grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create *colored* train and test datasets ETA 33min\n",
    "X_train_colored, y_train_colored, colored_train_num = create_input(local_path, mode=\"train\", grayscale=False)\n",
    "X_test_colored, y_test_colored, colored_test_num = create_input(local_path, mode=\"test\", grayscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to file\n",
    "\n",
    "# import json\n",
    "# with open(\"X_train\", \"w\") as fp:\n",
    "#     json.dump(X_train.tolist(), fp)\n",
    "\n",
    "# with open(\"y_train\", \"w\") as fp:\n",
    "#     json.dump(y_train.tolist(), fp)\n",
    "\n",
    "# with open(\"X_test\", \"w\") as fp:\n",
    "#     json.dump(X_test.tolist(), fp)\n",
    "\n",
    "# with open(\"y_test\", \"w\") as fp:\n",
    "#     json.dump(y_test.tolist(), fp)\n",
    "\n",
    "# with open(\"X_train_colored\", \"w\") as fp:\n",
    "#     json.dump(X_train_colored.tolist(), fp)\n",
    "\n",
    "# with open(\"y_train_colored\", \"w\") as fp:\n",
    "#     json.dump(y_train_colored.tolist(), fp)\n",
    "\n",
    "# with open(\"X_test_colored\", \"w\") as fp:\n",
    "#     json.dump(X_test_colored.tolist(), fp)\n",
    "\n",
    "# with open(\"y_test_colored\", \"w\") as fp:\n",
    "#     json.dump(y_test_colored.tolist(), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from file\n",
    "import json\n",
    "with open(\"X_train\", \"r\") as fp:\n",
    "    X_train = np.array(json.load(fp))\n",
    "\n",
    "with open(\"y_train\", \"r\") as fp:\n",
    "    y_train = np.array(json.load(fp))\n",
    "\n",
    "with open(\"X_test\", \"r\") as fp:\n",
    "    X_test = np.array(json.load(fp))\n",
    "\n",
    "with open(\"y_test\", \"r\") as fp:\n",
    "    y_test = np.array(json.load(fp))\n",
    "\n",
    "with open(\"X_train_colored\", \"r\") as fp:\n",
    "    X_train_colored = np.array(json.load(fp))\n",
    "\n",
    "with open(\"y_train_colored\", \"r\") as fp:\n",
    "    y_train_colored = np.array(json.load(fp))\n",
    "\n",
    "with open(\"X_test_colored\", \"r\") as fp:\n",
    "    X_test_colored = np.array(json.load(fp))\n",
    "\n",
    "with open(\"y_test_colored\", \"r\") as fp:\n",
    "    y_test_colored = np.array(json.load(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN base model\n",
    "cnn_model = models.Sequential()\n",
    "'''\n",
    "[1] convolution layer + [2] ReLU R(z) = max(0, z)\n",
    "filters 32 -> detect 32 different/features in the image, filter size 3 x 3\n",
    "'''\n",
    "cnn_model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "\n",
    "'''\n",
    "[3] pooling layer, looking for the max value in a 2 x 2 filter\n",
    "down-sampling operations that reduces the dimensionality of the feature map\n",
    "detect edges, corners\n",
    "'''\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "'''\n",
    "flattening - converting all the resultant 2d arrays from pooled feature map to a single long continuous linear vector\n",
    "'''\n",
    "cnn_model.add(layers.Flatten()) \n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "'''\n",
    "[4] fully connected layer to identify the numbers \n",
    "'''\n",
    "cnn_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Choose the optimal number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN base model\n",
    "model_epoch_diff = models.Sequential()\n",
    "model_epoch_diff.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model_epoch_diff.add(layers.MaxPooling2D((2, 2)))\n",
    "model_epoch_diff.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_epoch_diff.add(layers.MaxPooling2D((2, 2)))\n",
    "model_epoch_diff.add(layers.Flatten()) \n",
    "model_epoch_diff.add(layers.Dense(64, activation='relu'))\n",
    "model_epoch_diff.add(layers.Dense(10, activation='softmax'))\n",
    "model_epoch_diff.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def find_optimal_epoch(model, X_train, y_train, X_test, y_test, epoch_num):\n",
    "    test_loss_list, test_accuracy_list, train_loss_list, train_accuracy_list = [],[],[],[]\n",
    "    \n",
    "    for _ in range(1, epoch_num + 1):\n",
    "        h = model.fit(X_train, y_train, epochs=1)\n",
    "        train_loss = h.history['loss']\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy = h.history['accuracy']\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        test_loss, test_accuracy = model.evaluate(X_test,y_test)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "    return test_loss_list, test_accuracy_list, train_loss_list, train_accuracy_list\n",
    "\n",
    "test_loss_list, test_accuracy_list, train_loss_list, train_accuracy_list = find_optimal_epoch(model_epoch_diff, X_train, y_train, X_test, y_test, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, 16))\n",
    "plt.plot(x, test_loss_list, label=\"test_loss\") \n",
    "plt.plot(x, train_loss_list, label=\"train_loss\") \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Train/Test Loss vs. Number of Epochs\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, test_accuracy_list, label=\"test_accuracy\") \n",
    "plt.plot(x, train_accuracy_list, label=\"train_accuracy\") \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Train/Test Accuracy vs. Number of Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding**\n",
    "* Error rate increases when epoch >= 8\n",
    "* Early stop to avoid overfitting. \n",
    "* As the number of epochs increases beyond 8, test loss increases depicting the overfitting of the model on training data.\n",
    "* Choose epoch of 8\n",
    "* Ref: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Try different kernel sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kernel size 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a kernel of 3x3 size in the convolutional layer to see if accuracy improved\n",
    "model_3x3_kernel = models.Sequential()\n",
    "model_3x3_kernel.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model_3x3_kernel.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_3x3_kernel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_3x3_kernel.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_3x3_kernel.add(layers.Flatten()) \n",
    "model_3x3_kernel.add(layers.Dense(64, activation='relu'))\n",
    "model_3x3_kernel.add(layers.Dense(10, activation='softmax'))\n",
    "model_3x3_kernel.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3x3_kernel.fit(X_train, y_train, epochs=8)\n",
    "model_3x3_kernel.evaluate(X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel size 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It's a common practice to choose a odd-sized filter because the previous layer pixels will be semmertrical around the output pixel. \n",
    "* Choose between 3x3 and 5x5: \n",
    "* Comparing to 5x5 kernel size, 3x3 has a higher prediction accuracy (0.8658 than 0.8574) and costs less time (13ms/step than 16ms/step).\n",
    "* Thus, we will choose the 3x3 kernel size\n",
    "\n",
    "* Ref: https://medium.com/analytics-vidhya/significance-of-kernel-size-200d769aecb1#:~:text=For%20an%20odd%2Dsized%20filter,to%20suffer%20with%20aliasing%20error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Model with more/less layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 set of convolution + max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 set of convolution + pooling layers\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model1.add(layers.Flatten()) \n",
    "model1.add(layers.Dense(64, activation='relu'))\n",
    "model1.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train, y_train, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 sets of convolution + pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 sets of convolution + pooling layers\n",
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Flatten()) \n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train, y_train, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 sets of convolution + pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 sets of convolution + pooling layers\n",
    "\n",
    "model3 = models.Sequential()\n",
    "\n",
    "model3.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Flatten()) \n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit(X_train, y_train, epochs=8)\n",
    "loss_gray, accuracy_gray = model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_list, X_test, y_test):\n",
    "    lost_list, accuracy_list = [None], [None]\n",
    "    for m in model_list:\n",
    "        lost, accuracy = m.evaluate(X_test,y_test) \n",
    "        lost_list.append(lost)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    plt.xticks(range(1,4))\n",
    "    plt.plot(lost_list, label=\"lost_list\") \n",
    "    plt.plot(accuracy_list, label=\"accuracy_list\") \n",
    "    plt.legend()\n",
    "    plt.title(\"lost/accuracy vs. number of (conv + maxPooling) layers\")\n",
    "    plt.show()\n",
    "\n",
    "model_list = [model1, model2, model3]\n",
    "compare_models(model_list, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the graph above, the more layers in the model, the higher accuracy. \n",
    "* The time of training models is proportional to the number of layers. \n",
    "* From the perspective of time efficiency and accuracy, we will choose 3 sets of convolutional and maxPooling layers in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Colored vs Grayscale Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "model_colored = models.Sequential()\n",
    "model_colored.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model_colored.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_colored.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_colored.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_colored.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_colored.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_colored.add(layers.Flatten()) \n",
    "model_colored.add(layers.Dense(64, activation='relu'))\n",
    "model_colored.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model_colored.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_colored.fit(X_train_colored, y_train_colored, epochs=8)\n",
    "loss_colored, accuracy_colored = model_colored.evaluate(X_test_colored,y_test_colored) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar graph to compare colored images with \n",
    "fig = plt.figure(figsize = (3, 3))\n",
    "x = ['Grayscale', 'Colored']\n",
    "y = [accuracy_gray-0.5, accuracy_colored-0.5]\n",
    "plt.bar(x, y, color ='red',\n",
    "        width = 0.5, bottom = 0.5, align='center')\n",
    "\n",
    "plt.text(-0.08, 0.8, round(y[0]+0.5, 2))\n",
    "plt.text(0.92, 0.8, round(y[1]+0.5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a drop in the accuracy of using the colored image. Making the image grayscale helps improve the performance.\n",
    "* Classifying numbers are not related to color. The information from the grayscale image is enough for the classification.\n",
    "* Colored input will have three feature maps instead of one from the grayscaled image, which will potentially increase the chance of overfitting. It also increased the cost of computing. \n",
    "* Thus, we will use the grayscaled images in the model traning& testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=8)\n",
    "model.evaluate(X_test,y_test, verbose = 0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---DIVIDER---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions and save model\n",
    "y_pred = model.predict_classes(X_test.astype('float64'))\n",
    "model.save('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by y_pred, y_test and img_number\n",
    "print(y_pred.shape, y_test.shape, img_number.shape)\n",
    "\n",
    "grouped_dict_pred = {}\n",
    "grouped_dict_test = {}\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    img = img_number[i]\n",
    "    if img in grouped_dict_test:\n",
    "        grouped_dict_test[img].append(y_test[i])\n",
    "    else:\n",
    "        grouped_dict_test[img] = [y_test[i]]\n",
    "    \n",
    "    if img in grouped_dict_pred:\n",
    "        grouped_dict_pred[img].append(y_pred[i])\n",
    "    else:\n",
    "        grouped_dict_pred[img] = [y_pred[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count successful predicted images and calculate the accuracy\n",
    "cnt = 0\n",
    "for i in img_number:\n",
    "    print(grouped_dict_test[i])\n",
    "    if grouped_dict_pred[i] == grouped_dict_test[i]:\n",
    "        cnt += 1\n",
    "\n",
    "#Accuracy!\n",
    "acc = cnt / len(img_number)\n",
    "print(\"Accuracy for prediction is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a bar plot for grouped & separate prediction\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test,y_test, verbose = 0)\n",
    "fig = plt.figure(figsize = (3, 3))\n",
    "x = ['Individual', 'Grouped']\n",
    "y = [accuracy, acc]\n",
    "plt.bar(x, y, color ='red',\n",
    "        width = 0.5, align='center')\n",
    "\n",
    "plt.text(-0.08, 0.6, round(y[0], 2))\n",
    "plt.text(0.92, 0.6, round(y[1], 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======Plot cropped image======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cropped image with predicted& actual labels\n",
    "def plot_cropped_image(X, y, y_pred, idx):\n",
    "    plt.imshow(X[idx])\n",
    "    label = \"actual:\" + str(int(y[idx])) + \"\\npredicted:\" + str(y_pred[idx])\n",
    "    plt.xlabel(label)\n",
    "\n",
    "image_index = 1\n",
    "plot_cropped_image(X_test, y_test, y_pred, image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image with predicted& actual labels\n",
    "def display_image_with_pred(cnn_model, img_index, local_path):\n",
    "    mode = \"test\"\n",
    "    data = CropImage(local_path, mode)\n",
    "    img_dir = data.get_cropped_image_path(img_index - 1)\n",
    "    X = get_cropped_images(img_dir)\n",
    "    y = get_labels(data, img_index - 1)\n",
    "    y_pred = [np.argmax(y) for y in cnn_model.predict(np.array(X))]\n",
    "    \n",
    "    path = local_path + \"/\" + mode+\"/\" + str(img_index) + \".png\"\n",
    "    plt.imshow(img.imread(path))\n",
    "    \n",
    "    conclusion = \"Correct Prediction\" if y == y_pred else \"Incorrect Prediction\"\n",
    "    label = \"actual: \" + str(y) + \"\\npredicted: \" + str(y_pred) + \"\\n\" + conclusion\n",
    "    plt.xlabel(label)\n",
    "\n",
    "    title = \"{}.png\".format(img_index)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index = 12 \n",
    "display_image_with_pred(cnn_model, img_index, local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Accuracy, Precision, Recall, F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Precision = \\frac{TP}{TP+FP} $  \n",
    "\n",
    "$ Recall = \\frac{TP}{TP+FN} $  \n",
    "\n",
    "$ F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, F1-score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
